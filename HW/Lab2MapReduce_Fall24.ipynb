{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS/CMPSC 410 Fall 2024\n",
    "## Instructor: Professor John Yen\n",
    "## \n",
    "# Lab 2: MapReduce in Spark\n",
    "\n",
    "## The goals of this lab are for you to be able to\n",
    "## - Implement MapReduce using map, flatMap, and reduceByKey in Spark\n",
    "## - Apply the above for calculating total counts of words in a textfile.\n",
    "## - Be able to use .take to show contents of an RDD.\n",
    "## - Be able to generate an error message due to a typo in the input file.\n",
    "## Total Number of Exercises: \n",
    "- Exercise 1: 5 points\n",
    "- Exercise 2: 5 points\n",
    "- Exercise 3: 5 points\n",
    "- Exercise 4: 10 points\n",
    "- Exercise 5: 10 points\n",
    "- Exercise 6: 10 points\n",
    "- Exercise 7: 5 points\n",
    "## Total Points: 50 points\n",
    "\n",
    "# Due: midnight, September 8, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The first thing we need to do in each Jupyter Notebook running pyspark is to import pyspark first.\n",
    "# Note: You should have followed the instruction of Lab 2 (word document) to install PySpark in your ICDS account.  If not, complete all steps in the instructions before you proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Once we import pyspark, we need to import an important object called \"SparkContext\".  \n",
    "- Note: Every spark program needs a SparkContext object, which provides critical information (e.g., whether this Spark session runs in local mode or cluster mode) for the run-time environment of the Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We then create a Spark Context variable.  Once we have a spark context variable, we can execute spark codes. \n",
    "- The first parameter specifies that this Spark session is running in \"local\" mode. \n",
    "- The second parameter specifies a name for your Spark application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In creating the Spark Context variable below, we specified that this spark code is running in a \n",
    "`local`\n",
    "### mode, with a name\n",
    "`Lab2`.\n",
    "### After you run the cell below, be patient to wait for its completion before you \"run\" the next cell. When the left of the cell shows \n",
    "`[*]:`, \n",
    "### it means the 'run' is not completed yet. The completion of running a cell is indicated by a number in the brackets such as\n",
    "`[3]:`.\n",
    "### Note: You MUST wait for a cell to complete before you 'run' the next cell in PySpark Jupyter notebook. Otherwise, the execution of your later cells may generate errors because the inputs they need are not available yet (e.g., been not been generated by a previous PySpark statement that is still running)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/storage/icds/RISE/sw8/anaconda3-2021.11/conda_envs/pyspark/lib/python3.10/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/02 16:49:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://p-bc-5075.2e.hpc.psu.edu:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Lab2</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=Lab2>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc=SparkContext(\"local\", \"Lab2\")\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: You MUST wait for a cell to complete before you 'run\" the next cell in PySpark Jupyter notebook. Otherwise, the execution of your later cells may generate errors because the inputs they need are not available yet (e.g., have not been generated by a previous PySpark statement that is still running)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 (5 points) (a) Add your name below AND (b) replace the path below with the path of your home directory (i.e., replace juy1 with your PSU Access ID).\n",
    "## Answer for Exercise 1\n",
    "- a: Your Name: Daniel Woodford"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StayingSafeAtPennStateFall2024.txt MapPartitionsRDD[6] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_RDD = sc.textFile(\"StayingSafeAtPennStateFall2024.txt\")\n",
    "text_RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A: Parsing Input Text into Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD\n",
    "- RDD is the primary distributed data structure used by Spark for storing/accessing big data in a cluster.  \n",
    "- We will talk more about RDD next week. For now, we will view RDD as a big table.\n",
    "- When we use `textFile` of Spark to read an input file, it returns an RDD (a big table), where each entry in the big table is a line of the input file.\n",
    "- The contents of RDD can be obtained using `.take()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take\n",
    "- Take is an action, which we will discuss more next week.\n",
    "- Applying `take` method on an RDD shows the content of the RDD.  \n",
    "- The parameter of take is the number of entries in the RDD to be accessed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 (5%)\n",
    "Complete the code below to show the first five lines of the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '  Penn State <https://www.psu.edu/>',\n",
       " '',\n",
       " '  * Hotline <https://universityethics.psu.edu/hotline>',\n",
       " '  * Give <https://raise.psu.edu/?']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_RDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map\n",
    "- Map of Spark applies a function to an input RDD. We often use a `lambda` expression to describe an unnamed function as the parameter for map. \n",
    "- The body of the lambda expression below uses Python `strip` and `split` method (for string).\n",
    "- The Python `strip` method for strings removes spaces at the beginning and at the end of a string.\n",
    "- The Python `split` method for strings split the string using the specified delimiter (which is space `\" \"` in this case).\n",
    "- Returns an `RDD` (a \"big table\") where each row is the result of applying the map function to a corresponding row of the input RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[8] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_tokens_RDD = text_RDD.map(lambda line: line.strip().split(\" \"))\n",
    "line_tokens_RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 (5%)\n",
    "Complete the code below to show the first ten entries of the line_tokens_RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[''],\n",
       " ['Penn', 'State', '<https://www.psu.edu/>'],\n",
       " [''],\n",
       " ['*', 'Hotline', '<https://universityethics.psu.edu/hotline>'],\n",
       " ['*', 'Give', '<https://raise.psu.edu/?'],\n",
       " ['utm_source=psumain&utm_medium=referral&utm_campaign=DonateButton-'],\n",
       " ['MainNavigation>'],\n",
       " ['*', 'Apply', '<https://www.psu.edu/admission/>'],\n",
       " [''],\n",
       " ['*',\n",
       "  'This',\n",
       "  'is',\n",
       "  'Penn',\n",
       "  'State',\n",
       "  '<https://www.psu.edu/this-is-penn-state/>']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_tokens_RDD.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations:\n",
    "- Tokens/words in each entry in the text_RDD (i.e., each line in the input text file) are splitted/separated by the space character.\n",
    "- The `lambda` function returns a list of tokens for each line of the text_RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# flatMap \n",
    "The Spark `flatMap` method for RDD returns an RDD that removes the boundary between entries/rows of the input RDD big table.\n",
    "- Applying `flatMap` to `line_tokens_RDD` removes the boundary of different lines in the input text.  In another word, it merges the list of tokens for each lines in the text into a gigantic list of tokens for the entire input document.\n",
    "- Intuively, the effect of `flatMap` can be understood as **flattening** the internal structures of its input RDD.\n",
    "\n",
    "## Results of flatMap:\n",
    "We no longer see the list of tokens in `line_tokens_RDD` the reflects the line structure of the input file.  Instead, each token is an element of this gigantic list: `tokens_RDD`, which we will use to calculate the total number of occurance for each word/token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'Penn',\n",
       " 'State',\n",
       " '<https://www.psu.edu/>',\n",
       " '',\n",
       " '*',\n",
       " 'Hotline',\n",
       " '<https://universityethics.psu.edu/hotline>',\n",
       " '*',\n",
       " 'Give']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_RDD = line_tokens_RDD.flatMap(lambda x: x)\n",
    "tokens_RDD.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 (10%)\n",
    "- (a) Describe, based on the cotents of tokens_RDD shown by the previous statement, the effect of flatMap. (5%)\n",
    "- (b) What is the key difference between the output of map and flatmap ?(5%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers to Exercise 4:\n",
    "1. If we think of our original map as a 2D grid, flattening reduces the dimensionality to 1D by placing each token that is not on index 0 to a new line. For example, instead of ['Penn', 'State'] being one row, it is now ['Penn',\n",
    " 'State']\n",
    "\n",
    "2. The output is now one dimensional with flatmap instead of two dimensional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ttx7g16JsImQ"
   },
   "source": [
    "# Part B: Counting Word Frequency using MapReduce\n",
    "\n",
    "### We want to count the total number of time a word/token occurs in the twitter dataset. We can use the concept of MapReduce to do this in a \"scalable\" way such that we can do this calculation even if the size of twitter dataset is too large to fit into a computer.\n",
    "\n",
    "### A MapReduce way to achieve this involves two steps:\n",
    "### Step 1: map each word into a key value pair \n",
    "`(<word>, 1)`\n",
    "### The key of this key-value pair is the word (in the input RDD); the value of the key-value pair is the number 1.\n",
    "### Step 2: Use reduceByKey in Spark to aggregate all pairs of the same key into\n",
    "`(<word>, <count>)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Lambda function\n",
    "The lambda function that is the parameter of `map` is applied to each element of the input RDD (which is each token in the tokens_RDD). Lambda function (in Python) has the format of\n",
    "`lambda <parameter list>: <function body>`\n",
    "The value returned by the lambda function is either \n",
    "- an explicit `return` statement, or\n",
    "- the value returned by the last statement in the body of the function.\n",
    "\n",
    "In the example below, the lambda function returns the key value pair `(<input word>, 1)`\n",
    "because it is the only (hence also the last) statement in the body of the lambda\n",
    "expression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[11] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_1_RDD = tokens_RDD.map(lambda x: (x, 1))\n",
    "token_1_RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 1),\n",
       " ('Penn', 1),\n",
       " ('State', 1),\n",
       " ('<https://www.psu.edu/>', 1),\n",
       " ('', 1),\n",
       " ('*', 1),\n",
       " ('Hotline', 1),\n",
       " ('<https://universityethics.psu.edu/hotline>', 1),\n",
       " ('*', 1),\n",
       " ('Give', 1)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_1_RDD.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reduceByKey\n",
    "\n",
    "The transformation `reduceByKey` also takes a lambda function. However, this function takes TWO parameters.\n",
    "- The first parameter `lambda x,y: x+y` is an aggregation function, which has two parameters (x and y in the code below).\n",
    "- The first parameter of reduceByKey x is a counter (initial value is 0).\n",
    "- The second parameter of reduceByKey y is the 'value' in input key-value pairs to be aggregated with the counter.  \n",
    "- Since the 'values' in the input key-value pairs are all 1, this lambda function simplies increment the accumulator for each occurance of the key.  Because reduceByKey aggregates across the entire input RDD, the final value (i.e., accumulator) for each word/token is the total number it occurs in the Twitter dataset.\n",
    "- The second parameter is the number of partitions used to partition the keys (so that the reduce task can be distributed among multiple reduce workers for scalability). In the cluster mode, the choice of this number should include considerations about the total number of nodes available in the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5 (10 points) Complete the code below by choosing a name for the RDD to be generated by `reduceByKey`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 143),\n",
       " ('Penn', 38),\n",
       " ('State', 30),\n",
       " ('<https://www.psu.edu/>', 2),\n",
       " ('*', 65),\n",
       " ('Hotline', 2),\n",
       " ('<https://universityethics.psu.edu/hotline>', 1),\n",
       " ('Give', 1),\n",
       " ('<https://raise.psu.edu/?', 1),\n",
       " ('utm_source=psumain&utm_medium=referral&utm_campaign=DonateButton-', 1)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_1_RBK = token_1_RDD.reduceByKey(lambda x,y: x+y)\n",
    "token_1_RBK.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6 (10 points) Complete the code below by filling in the path of your Tweets word count output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# saveAsTextFile\n",
    "This action saves the content of the input RDD in the directory specified.  Even though we add \".txt\" extension to the directory (to remind ourself it contains text file, not CSV),\n",
    "the path is used as a directory to store all partitions of the given RDD (i.e., `token_count_RDD` in this lab). \n",
    "\n",
    "## An important requirement for saveAsTextFile is the path for the output files SHOULD NOT EXIST.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging Tip #1: \n",
    "Before you run saveAsTextFile each time, double check whether the output path used does not already exist.  If it does, either \n",
    "- (a) remove the directory, or\n",
    "- (b) change the output path to one that has not been used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"Lab2_output.txt\"\n",
    "token_1_RBK.saveAsTextFile(output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7 (5 points) Modify the path of input file to create a typo in the INPUT FILE NAME.  Execute the code below. \n",
    "- You do not need to fix the typo.\n",
    "- (a) When do you expect to see the first error message before running the code below?\n",
    "- (b) When does the error message actually show up?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "notreal.txt MapPartitionsRDD[22] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2_RDD = sc.textFile(\"notreal.txt\")\n",
    "text2_RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "line2_RDD = text2_RDD.map(lambda line: line.strip().split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2_RDD = line2_RDD.flatMap(lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[23] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2_RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2_1_RDD = token2_RDD.map(lambda x: (x, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o136.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/storage/home/dfw5416/Lab2/notreal.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.IOException: Input path does not exist: file:/storage/home/dfw5416/Lab2/notreal.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t... 25 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m token2_count_RDD \u001b[38;5;241m=\u001b[39m \u001b[43mtoken2_1_RDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduceByKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/storage/icds/RISE/sw8/anaconda/conda_envs/pyspark/lib/python3.10/site-packages/pyspark/rdd.py:1893\u001b[0m, in \u001b[0;36mRDD.reduceByKey\u001b[0;34m(self, func, numPartitions, partitionFunc)\u001b[0m\n\u001b[1;32m   1875\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreduceByKey\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, numPartitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, partitionFunc\u001b[38;5;241m=\u001b[39mportable_hash):\n\u001b[1;32m   1876\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1877\u001b[0m \u001b[38;5;124;03m    Merge the values for each key using an associative and commutative reduce function.\u001b[39;00m\n\u001b[1;32m   1878\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1891\u001b[0m \u001b[38;5;124;03m    [('a', 2), ('b', 1)]\u001b[39;00m\n\u001b[1;32m   1892\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1893\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombineByKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumPartitions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartitionFunc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/storage/icds/RISE/sw8/anaconda/conda_envs/pyspark/lib/python3.10/site-packages/pyspark/rdd.py:2138\u001b[0m, in \u001b[0;36mRDD.combineByKey\u001b[0;34m(self, createCombiner, mergeValue, mergeCombiners, numPartitions, partitionFunc)\u001b[0m\n\u001b[1;32m   2094\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2095\u001b[0m \u001b[38;5;124;03mGeneric function to combine the elements for each key using a custom\u001b[39;00m\n\u001b[1;32m   2096\u001b[0m \u001b[38;5;124;03mset of aggregation functions.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2135\u001b[0m \u001b[38;5;124;03m[('a', [1, 2]), ('b', [1])]\u001b[39;00m\n\u001b[1;32m   2136\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m numPartitions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2138\u001b[0m     numPartitions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_defaultReducePartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2140\u001b[0m serializer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39mserializer\n\u001b[1;32m   2141\u001b[0m memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_memory_limit()\n",
      "File \u001b[0;32m/storage/icds/RISE/sw8/anaconda/conda_envs/pyspark/lib/python3.10/site-packages/pyspark/rdd.py:2583\u001b[0m, in \u001b[0;36mRDD._defaultReducePartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39mdefaultParallelism\n\u001b[1;32m   2582\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2583\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetNumPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/storage/icds/RISE/sw8/anaconda/conda_envs/pyspark/lib/python3.10/site-packages/pyspark/rdd.py:2937\u001b[0m, in \u001b[0;36mPipelinedRDD.getNumPartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2936\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetNumPartitions\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 2937\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prev_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msize()\n",
      "File \u001b[0;32m/storage/icds/RISE/sw8/anaconda/conda_envs/pyspark/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/storage/icds/RISE/sw8/anaconda/conda_envs/pyspark/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o136.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/storage/home/dfw5416/Lab2/notreal.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.IOException: Input path does not exist: file:/storage/home/dfw5416/Lab2/notreal.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t... 25 more\n"
     ]
    }
   ],
   "source": [
    "token2_count_RDD = token2_1_RDD.reduceByKey(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer to Exercise 7 (10 points):\n",
    "### Type your answer to Exercise 7 below:\n",
    "- (a) My guess is going to be the 4th code chunk in that section since you're asking me. Perhaps you can create a map and flatten it out of nothing but printing the lack of anything will cause the error.\n",
    "- (b) The error showed up at the last code chunk saying that the path does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `.stop()` method terminates the given SparkContext. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
