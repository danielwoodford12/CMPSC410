{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS/CMPSC 410 Fall 2024\n",
    "# Instructor: Professor John Yen\n",
    "# TA: Jin Peng and Al Lawati, Ali Hussain Mohsin\n",
    "# Lab 6: Movie Recommendations Using Alternative Least Square\n",
    "## The goals of this lab are for you to be able to\n",
    "### - Use Alternating Least Squares (ALS) for recommending movies based on reviews of users\n",
    "### - Be able to understand the raionale for splitting data into training, validation, and testing.\n",
    "### - Be able to use MLlib to implement an ALS-based movie recommendation system.\n",
    "### - Be able to use RDD transformations to calculate training error, validation error, and prediction error of a model.\n",
    "### - Be able to tune hyper-parameters of the ALS model using a small dataset (in local mode)\n",
    "### - Be able to store the results of evaluating hyper-parameters using Pandas dataframe.\n",
    "### - Be able to automatically identify the best hyper-parameters and evaluate the chosen model with testing data.\n",
    "### - Be able to use CheckPoint to improve the efficiency of iterative processing using Spark.\n",
    "### - Be able to debug (in local mode) using Restart Kernel if needed.\n",
    "### - Note: This lab only requires running Spark in the local mode (using small review dataset). Lab 7 will require running ALS-based recommendation on large review dataset in the cluster mode.\n",
    "\n",
    "## Exercises: \n",
    "- Exercise 1: 5 points\n",
    "- Exercise 2A: 5 points\n",
    "- Exercise 2B: 10 points\n",
    "- Exercise 3: 10 points\n",
    "- Exercise 4: 10 points\n",
    "- Exercise 5: 10 points\n",
    "- Exercise 6: 25 points\n",
    "- Exercise 7: 15 points\n",
    "- Exercise 8: 10 points\n",
    "## Total Points: 100 points\n",
    "\n",
    "# Due: midnight, Oct 7th, 2024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The first thing we need to do in each Jupyter Notebook running pyspark is to import pyspark first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once we import pyspark, we need to import \"SparkContext\".  Every spark program needs a SparkContext object\n",
    "### In order to use Spark SQL on DataFrames, we also need to import SparkSession from PySpark.SQL\n",
    "### In addition, we import ``ALS`` from ``MLlib.recommendation`` of ``pyspark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType, IntegerType, FloatType\n",
    "from pyspark.sql.functions import col, column\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql import Row\n",
    "from pyspark.mllib.recommendation import ALS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We then create a Spark Session variable (rather than Spark Context) in order to use DataFrame. \n",
    "- Note: We temporarily use \"local\" as the parameter for master in this notebook so that we can complete and debug the code using Jupyter Server.  After we export it to .py file for execution in the cluster, however, we need to REMOVE .master(\"local\") in the .py file so that it runs in cluster (Standalone) mode in ICDS cluster when we execute ``pbs-spark-submit``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/storage/icds/RISE/sw8/anaconda3-2021.11/conda_envs/pyspark/lib/python3.10/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/30 15:49:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/09/30 15:49:56 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "ss=SparkSession.builder.master(\"local\").appName(\"Lab6 ALS-based Recommendation Systems\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.sparkContext.setCheckpointDir(\"~/scratch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 (5 points) (a) Add your name below AND (b) replace the path below with the path of your home directory.\n",
    "## Answer for Exercise 1\n",
    "- a: Your Name: Daniel Woodford"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_schema = StructType([ StructField(\"UserID\", IntegerType(), False ), \\\n",
    "                            StructField(\"MovieID\", IntegerType(), True), \\\n",
    "                            StructField(\"Rating\", FloatType(), True ), \\\n",
    "                            StructField(\"RatingID\", IntegerType(), True ), \\\n",
    "                           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_DF = ss.read.csv(\"ratings_samples.csv\", schema=rating_schema, header=True, inferSchema=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- UserID: integer (nullable = true)\n",
      " |-- MovieID: integer (nullable = true)\n",
      " |-- Rating: float (nullable = true)\n",
      " |-- RatingID: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings_DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings2_DF = ratings_DF.select(\"UserID\",\"MovieID\",\"Rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(UserID=1, MovieID=31, Rating=2.5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings2_DF.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notice that the content of DataFrame is a ``Row`` object.  \n",
    "## The ``Row`` object of a DataFrame enables us to access different column values of the rows by the name of their column, as we shall see later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting a PySpark DataFrame into an RDD\n",
    "- Just use ``<DataFrame>.rdd``, which returns an RDD representation of the ``<DataFrame>``\n",
    "- Notice also that the content of the RDD is a list of ``Row`` object.  As we will see later, you can access the contents of the RDD in three ways.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(UserID=1, MovieID=31, Rating=2.5),\n",
       " Row(UserID=1, MovieID=1029, Rating=3.0),\n",
       " Row(UserID=1, MovieID=1061, Rating=3.0)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings2_RDD = ratings2_DF.rdd\n",
    "ratings2_RDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 Split Data into Three Sets: Training Data, Evaluatiion Data, and Testing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2A (5 points)\n",
    "## Complete the code below to split `ratings2_RDD` into three groups: 60% training, 20% validation, and 20% testing.\n",
    "## ``randomSplit`` is an RDD method that takes a list of weights/percentages used to split the input RDD. The second parameter is a seed for random number generator (typically a prime number)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_RDD, validation_RDD, test_RDD = ratings2_RDD.randomSplit([.6, .2, .2], 19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation Systems\n",
    "The recommendation system in this lab generates a predicted \"rating\" for a given user and a given movie.  Therefore, the input of the system is a list of two items: a UserID and a Movie ID.\n",
    "## Prepare input (UserID, MovieID) for training, validation and for testing data\n",
    "Each entry of the ``ratings2_RDD`` is a ``Row`` object, containing three elements: UserID, MovieID, and Rating.\n",
    "### We extract the first two elements of each Row in ``traing_RDD``, ``validation_RDD``, and ``test_RDD``to create the input for training, validation, and testing, respectively.\n",
    "### We will use these RDD in evaluating the model later.  \n",
    "### We will first construct an ALS recommedation model using training_RDD, which includes both input (UserID, MovieID) and desired prediction output (Rating)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input_RDD = training_RDD.map(lambda x: (x[0], x[1]) )\n",
    "validation_input_RDD = validation_RDD.map(lambda x: (x[0], x[1]) ) \n",
    "testing_input_RDD = test_RDD.map(lambda x: (x[0], x[1]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 Construct a Movie Recommendation Model \n",
    "## We use ``ALS`` from `PySpark.MLlib.recommendation` module and training data to train a recommendation model.\n",
    "## Exercise 2B (10 points)\n",
    "## Choose a rank between 3 and 6, a randon number for the seed, 30 iterations, and 0.1 regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model = ALS.train(training_RDD, 4, seed=17, iterations=30, lambda_=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2.1 Compute Training Error of the ALS recommendation model\n",
    "### We use the training_input_RDD we prepared earlier to generate predicted ratings for ``(<userID>, <movieID>)`` pairs in the training dataset so that we can compare their predicted ratings with their actual ratings to calculate the error (for training data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "training_prediction_RDD = model.predictAll(training_input_RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Rating(user=253, product=37739, rating=3.4122745865216526),\n",
       " Rating(user=402, product=69069, rating=3.904311960189694),\n",
       " Rating(user=213, product=44828, rating=0.4832854965056512),\n",
       " Rating(user=428, product=5618, rating=4.406297204322684)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_prediction_RDD.take(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notice: The output of ALS has predefined column names: ``user``, ``product``, and ``rating``, which are different from the column names of the training data.\n",
    "## Write down these column names.  You will need them in Exercise 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining two RDDs on keys\n",
    "Two key-value RDDs can be joined on their keys so that entries of the two RDDs with common keys are combined in the RDD generated by the join operation.\n",
    "# Joining two RDDs on keys for Evaluating Recommendation Systems\n",
    "To evaluate a recommendation system, we need to combine, for each ``(<user> <movie>)`` pair, its predicted rating and its actual rating.\n",
    "This can be achieved in four steps:\n",
    "- First, convert the Training_RDD (or Evaluation_RDD) from the format of ``(<user>, <movie>, <actual_rating>)`` into a key-value format: ``( (<user>, <movie>), <actual_rating> )``.  Notice that the key is now a list of two elements: ``<user>`` and ``<movie>``.\n",
    "- Second, convert training_prediction_RDD also from the format of ``(<user>, <movie>, <predicted_rating>)`` to an identical key-value format: ``( (<user>, <movie>), <predicted_rating> )``\n",
    "- Third, join the two RDDs on their keys, resulting in a new key-value RDD in the format of ``( (<user>, <movie> ), ( <actual_ratings>, <predicted_ratings> ) )``\n",
    "- Fourth, calculate the errors between predicted ratings and actual ratings across all users and movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three Ways to Access Elements of a 'Row' object in an RDD (generated from Spark DataFrame)\n",
    "Before we perform the three steps above, we are going to first learn three methods for accessing elements of a Row object in an RDD (generated from Spark DataFrame).  Mastering these methods will enable you to, not only implement the three steps above, but also be able to apply them to a wide range of situations where you need to transform the structure of RDDs (e.g., for integrating data, for evaluating models, etc).\n",
    "\n",
    "For example, you will later use these methods to implment step 1 and step 2 above, and to calculate model prediction errors after step 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(UserID=1, MovieID=31, Rating=2.5),\n",
       " Row(UserID=1, MovieID=1263, Rating=2.0),\n",
       " Row(UserID=1, MovieID=1343, Rating=2.0)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_RDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Access elements of a row using column name of the DataFrame (from which the RDD came from) using the syntax ``<Row variable>[ <ColumnName> ]``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_target_output_RDD = training_RDD.map(lambda x: ( (x['UserID'], x['MovieID']), x['Rating'] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 31), 2.5), ((1, 1263), 2.0), ((1, 1343), 2.0)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_target_output_RDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice the transformed RDD above has ``(<userID), <movieID>)`` as keys, and their ratings as values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Access elements of a row using column name (that does not contain space) of the DataFrame schema (from which the RDD came from) using the syntax ``<Row variable>.<ColumnName>``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_target_output2_RDD = training_RDD.map(lambda x: ( ( x.UserID, x.MovieID ), x.Rating ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 31), 2.5), ((1, 1263), 2.0), ((1, 1343), 2.0)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_target_output2_RDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice the transformed RDD using this second method has identical content as that generated by the first method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Access elements of a row using column name of the DataFrame (from which the RDD came from) using the syntax ``<row variable>[<index>]`` where ``<index>`` is the integer that indicates the position of the element in the row (starting with 0 for the first element)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_target_output3_RDD = training_RDD.map(lambda x: ( (x[0], x[1]), x[2] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 31), 2.5), ((1, 1263), 2.0), ((1, 1343), 2.0)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_target_output3_RDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is the result of the transformed RDD using this method similar to the two above?  \n",
    "Yes, the result is the same\n",
    "### What is the benefit of the first two methods?\n",
    "In the first two methods it may be easier or more comprehensible to use the column name rather than the index of the column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 (10 points)\n",
    "## Use method 2 (for accessing content of RDD) to complete the code below to transform the ALS model prediction of training data into the format of `( (<user> <product>), <rating> )` so that we can later join it with training target outpt RDD for computing Root Mean Square Error of predictions.\n",
    "### Notice: The "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_prediction2_RDD = training_prediction_RDD.map(lambda r: ( (r.user, r.product), r.rating ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[((253, 37739), 3.4122745865216526),\n",
       " ((402, 69069), 3.904311960189694),\n",
       " ((213, 44828), 0.4832854965056512)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_prediction2_RDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_evaluation_RDD = training_target_output_RDD.join(training_prediction2_RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[((1, 31), (2.5, 2.5728871712524892)),\n",
       " ((1, 1263), (2.0, 2.2544346894707754)),\n",
       " ((1, 1343), (2.0, 2.1853108164581343))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_evaluation_RDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Because the joined RDD is no longer a Row object, you can access its content using the third method above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For an RDD containing numbers, we can use the following actions to calculate their min, max, sum, and mean:\n",
    "- ``<RDD>.min``: returns the minimum value in the RDD.\n",
    "- ``<RDD>.max``: returns the maximal value in the RDD.\n",
    "- ``<RDD>.sum``: returns the sum of all values in the RDD.\n",
    "- ``<RDD>.mean``: returns the mean of all values in the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "training_error = math.sqrt(training_evaluation_RDD.map(lambda z: (z[1][0] - z[1][1])**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6486012428396611\n"
     ]
    }
   ],
   "source": [
    "print(training_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2.2 Compute Validation Errors\n",
    "In a similar way, we can calculate the validation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_prediction_RDD = model.predictAll(validation_input_RDD).map(lambda x: ( (x.user, x.product), x.rating ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((48, 44828), 0.40229039645299036),\n",
       " ((331, 5618), 4.3199181947106755),\n",
       " ((577, 5618), 4.926704876628129)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_prediction_RDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4 (10 points)\n",
    "## Complete the code below to transform `validation_RDD` into the same key value pair format of Exercise 3, then join it with `validation_prediction_RDD` to prepare for RMS error calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_target_RDD = validation_RDD.map(lambda y: ( (y.UserID, y.MovieID), y.Rating ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_evaluation_RDD = validation_target_RDD.join(validation_prediction_RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[((1, 1129), (2.0, 2.0247077674322007)),\n",
       " ((1, 1287), (2.0, 2.241996889286977)),\n",
       " ((2, 52), (3.0, 3.5191739609321457))]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_evaluation_RDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5 (10 points)\n",
    "## Complete the code below to calculate RMS error for validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_error_RDD = validation_evaluation_RDD.map(lambda z: (z[1][0] - z[1][1])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0006104737714837181, 0.05856249442457332, 0.2695416017099732]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_error_RDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_RMS_error = math.sqrt(validation_error_RDD.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9393572722599206\n"
     ]
    }
   ],
   "source": [
    "print(validation_RMS_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How is the validation error compared to the training error?\n",
    "The validation error is much larger than training error\n",
    "### Is this what you expected?\n",
    "To an extent; validation error tends to be higher as it is new data rather than what the model trained on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3 Hyperparameter Tuning\n",
    "## Iterate through all possible combination of a set of values for three hyperparameters for ALS Recommendation Model:\n",
    "- rank (k)\n",
    "- regularization\n",
    "- iterations \n",
    "## Each hyperparameter value combination is used to construct an ALS recommendation model using training data, but evaluate using Evaluation Data\n",
    "## The evaluation results are saved in a Pandas DataFrame \n",
    "``\n",
    "hyperparams_eval_df\n",
    "``\n",
    "## The best hyperprameter value combination is stored in 4 variables\n",
    "``\n",
    "best_k, best_regularization, best_iterations, and lowest_validation_error\n",
    "``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7 (25 points) \n",
    "## Complete the code below (using answers to Exercise 3, 4, and 5) to iterate through a set of hyperparameters (rank k, regularization parameter, and number of iterations) to create and evaluate ALS recommendation models to find the best model among all those created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12392:===========================>                           (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best rank k is  10 , regularization =  0.2 , iterations =  15 . Validation Error = 0.9191682688219361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## Initialize a Pandas DataFrame to store evaluation results of all combination of hyper-parameter settings\n",
    "hyperparams_eval_df = pd.DataFrame( columns = ['k', 'regularization', 'iterations', 'validation RMS', 'testing RMS'] )\n",
    "# initialize index to the hyperparam_eval_df to 0\n",
    "index =0 \n",
    "# initialize lowest_error\n",
    "lowest_validation_error = float('inf')\n",
    "# Set up the possible hyperparameter values to be evaluated\n",
    "iterations_list = [15, 30]\n",
    "regularization_list = [0.1, 0.2, 0.3]\n",
    "rank_list = [4, 7, 10, 13]\n",
    "for k in rank_list:\n",
    "    for regularization in regularization_list:\n",
    "        for iterations in iterations_list:\n",
    "            seed = 37\n",
    "            # Construct a recommendation model using a set of hyper-parameter values and training data\n",
    "            model = ALS.train(training_RDD, rank=k, seed=seed, iterations=iterations, lambda_=regularization)\n",
    "            # Evaluate the model using evalution data\n",
    "            # map the output into ( (user, product), rating ) so that we can join with actual evaluation data\n",
    "            # using (userID, movieID) as keys.\n",
    "            validation_prediction_RDD= model.predictAll(validation_input_RDD).map(lambda x: (  (x.user, x.product), x.rating )    )\n",
    "            validation_evaluation_RDD = validation_RDD.map(lambda y: ( (y.UserID, y.MovieID), y.Rating ) ).join(validation_prediction_RDD)\n",
    "            # Calculate RMS error between the actual rating and predicted rating for (userID, movieID) pairs in validation dataset\n",
    "            validation_error = math.sqrt(validation_evaluation_RDD.map(lambda z: (z[1][0] - z[1][1])**2).mean())\n",
    "            # Save the error as a row in a pandas DataFrame\n",
    "            hyperparams_eval_df.loc[index] = [k, regularization, iterations, validation_error, float('inf')]\n",
    "            index = index + 1\n",
    "            # Check whether the current error is the lowest\n",
    "            if validation_error < lowest_validation_error:\n",
    "                best_k = k\n",
    "                best_regularization = regularization\n",
    "                best_iterations = iterations\n",
    "                best_index = index - 1\n",
    "                lowest_validation_error = validation_error\n",
    "print('The best rank k is ', best_k, ', regularization = ', best_regularization, ', iterations = ',\\\n",
    "      best_iterations, '. Validation Error =', lowest_validation_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Testing Data to Evaluate the Model built using the Best Hyperparameters                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.4 Evaluate the best hyperparameter combination using testing data\n",
    "## If the error between rating prediction and actual rating for (userID, movie ID) pairs in the training data is comparable to the error of validation data, our model passes the test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8 (15 points)\n",
    "- (a) Complete the code below to evaluate the best hyperparameter combinations (saved in variables such as ``best_k`` in the previous iteration) using testing data. (10 point)\n",
    "- (b) Does your model performance for testing data comparable to the performance using validation data?  Explain your answer in the Markdown cell below. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13016:===========================>                           (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Testing Error for rank k = 10  regularization =  0.2 , iterations =  15  is :  0.9217817274717887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "model = ALS.train(training_RDD, best_k, seed= 19, iterations=best_iterations, lambda_=best_regularization)\n",
    "testing_prediction_RDD=model.predictAll(testing_input_RDD).map(lambda x: ((x.user, x.product), x.rating) )\n",
    "testing_evaluation_RDD= test_RDD.map(lambda x: ((x.UserID, x.MovieID), x.Rating)).join(testing_prediction_RDD)\n",
    "testing_error = math.sqrt(testing_evaluation_RDD.map(lambda x: (x[1][0]-x[1][1])**2).mean())\n",
    "print('The Testing Error for rank k =', best_k, ' regularization = ', best_regularization, ', iterations = ', \\\n",
    "      best_iterations, ' is : ', testing_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer to Exercise 8: \n",
    "- (b) Yes, the model performance is comparable with testing as compared to validation data. The errors are relatively similar, being .919 for validation and .922 for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "print(best_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the Testing RMS in the DataFrame\n",
    "hyperparams_eval_df.loc[best_index]=[best_k, best_regularization, best_iterations, lowest_validation_error, testing_error]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema3= StructType([ StructField(\"k\", FloatType(), True), \\\n",
    "                      StructField(\"regularization\", FloatType(), True ), \\\n",
    "                      StructField(\"iterations\", FloatType(), True), \\\n",
    "                      StructField(\"Validation RMS\", FloatType(), True), \\\n",
    "                      StructField(\"Testing RMS\", FloatType(), True) \\\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the pandas DataFrame that stores validation errors of all hyperparameters and the testing error for the best model to a Spark DataFrame, so that it can be written in the cluster mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "HyperParams_RMS_DF = ss.createDataFrame(hyperparams_eval_df, schema3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9 (10 points)\n",
    "## Modify the output path so that your output results can be saved in a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"Lab6ALSHyperParamsTuning9_22\"\n",
    "HyperParams_RMS_DF.write.option(\"header\", True).csv(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
