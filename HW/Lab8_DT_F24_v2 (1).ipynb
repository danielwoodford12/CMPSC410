{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS/CMPSC 410 Fall 2024\n",
    "# Instructor: Professor John Yen\n",
    "# TAs: Jin Peng and Al Lawati, Ali Hussain Mohsin\n",
    "\n",
    "## Lab 8 Decision Tree Learning Using ML Pipeline, Visualization, and Hyperparameter Tuning\n",
    "\n",
    "## The goals of this lab are for you to be able to\n",
    "- Understand the function of the different steps/stages involved in Spark ML pipeline\n",
    "- Be able to construct a decision tree using Spark ML machine learning module\n",
    "- Be able to generate a visualization of Decision Trees\n",
    "- Be able to perform automated hyper-parameter tuning for Decision Trees \n",
    "\n",
    "## The data set used in this lab is a Breast Cancer diagnosis dataset.\n",
    "\n",
    "## Submit the following items for Lab 8 (DT)\n",
    "- Completed Jupyter Notebook of Lab 8 (in HTML format)\n",
    "- A visualization of the decision tree generated in Part 5.\n",
    "- The output file that contains the best DT hyperparameters for Part 6.\n",
    "- A visualization of the best decision tree generated in Part 6.\n",
    "- The output file that contains the best DT hyperparameters for Part 7.\n",
    "- a visualization of the best decision tree generated in Part 7.\n",
    "\n",
    "## Total Number of Exercises: 8\n",
    "- Exercise 1: 5 points\n",
    "- Exercise 2: 5 points\n",
    "- Exercise 3: 10 points  \n",
    "- Exercise 4: 10 points \n",
    "- Exercise 5: 20 points\n",
    "- Exercise 6: 10 points\n",
    "- Exercise 7: 30 points\n",
    "- Exercise 8: 10 points\n",
    "## Total Points: 100 points\n",
    "\n",
    "# Due: midnight, October 29, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and set up the Python files for this Lab based on the instructions in \"SpecialInstructionsLab 8\" in Canvas (under Module Topic and Lab 8)\n",
    "1. Create a \"Lab8DT\" directory in the work directory of your work directory.\n",
    "2. Create a subdirectory under \"Lab8DT\" called \"decision_tree_plot\" (named the directory EXACTLY this way).\n",
    "3. Upload the following three files in Module 8 from Canvas to the decision_tree_plot subdirectory\n",
    "- decision_tree_parser.py\n",
    "- decision_tree_plot.py\n",
    "- tree_template.jinjia2\n",
    "4. After you have completed the steps above, upload the Jupyter Notebook for Lab 8 (Lab8_DT_F24.ipynb) to the Lab8DT directory.\n",
    "5. Upload the data file breast-cancer-wisconsin.data.txt from module 8 in Canvas to the \"Lab8DT\" directory.\n",
    "6. Open the Jupyter Notebook for Lab 8 and follow the instructions to complete the lab.\n",
    "\n",
    "# Follow the instructions below and execute the PySpark code cell by cell below. Make modifications as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notice that we use PySpark SQL module to import SparkSession because ML works with SparkSession\n",
    "## Notice also the different methods imported from ML and three submodules of ML: classification, feature, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType, IntegerType, FloatType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, IndexToString\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following two lines import relevant functions from the two python files you uploaded into the decision_tree_plot subdirectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decision_tree_plot.decision_tree_parser import decision_tree_parse\n",
    "from decision_tree_plot.decision_tree_plot import plot_trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This lab runs Spark in the local mode because the size of data is small. \n",
    "## When you need to develop a Decision Tree-based Predictive Model for a large dataset, you will need to debug the code in local mode using a small sampled data.  After running in local mode successfully, you will need to convert it for cluster mode for running using the large dataset, like previous labs.\n",
    "### Notice we are creating a SparkSession, not a SparkContext, when we use ML pipeline.\n",
    "### The \"getOrCreate()\" method means we can re-evaluate this without a need to \"stop the current SparkSession\" first (unlike SparkContext)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/storage/icds/RISE/sw8/anaconda3-2021.11/conda_envs/pyspark/lib/python3.10/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/22 20:33:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "ss=SparkSession.builder.master(\"local\").appName(\"lab 8 DT\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: (5 points) Enter your name below:\n",
    "- My Name: Daniel Woodford"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As we have seen in previous labs, we can define the schema for reading the input data into a PySpark DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: (5 points) Complete the following path with the path for your home directory.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_schema = StructType([ StructField(\"id\", IntegerType(), False ), \\\n",
    "                        StructField(\"clump_thickness\", IntegerType(), False), \\\n",
    "                        StructField(\"unif_cell_size\", IntegerType(), False ), \\\n",
    "                        StructField(\"unif_cell_shape\", IntegerType(), False ), \\\n",
    "                        StructField(\"marg_adhesion\", IntegerType(), False), \\\n",
    "                        StructField(\"single_epith_cell_size\", IntegerType(), False), \\\n",
    "                        StructField(\"bare_nuclei\", IntegerType(), False),\\\n",
    "                        StructField(\"bland_chrom\", IntegerType(), False), \\\n",
    "                        StructField(\"norm_nucleoli\", IntegerType(), False), \\\n",
    "                        StructField(\"mitoses\", IntegerType(), False), \\\n",
    "                        StructField(\"class\", StringType(), False) \\\n",
    "                           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ss.read.csv(\"breast-cancer-wisconsin.data.txt\", schema=bc_schema, header=True, inferSchema=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 Feature Transformation Using DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- clump_thickness: integer (nullable = true)\n",
      " |-- unif_cell_size: integer (nullable = true)\n",
      " |-- unif_cell_shape: integer (nullable = true)\n",
      " |-- marg_adhesion: integer (nullable = true)\n",
      " |-- single_epith_cell_size: integer (nullable = true)\n",
      " |-- bare_nuclei: integer (nullable = true)\n",
      " |-- bland_chrom: integer (nullable = true)\n",
      " |-- norm_nucleoli: integer (nullable = true)\n",
      " |-- mitoses: integer (nullable = true)\n",
      " |-- class: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+--------------+---------------+-------------+----------------------+-----------+-----------+-------------+-------+-----+\n",
      "|     id|clump_thickness|unif_cell_size|unif_cell_shape|marg_adhesion|single_epith_cell_size|bare_nuclei|bland_chrom|norm_nucleoli|mitoses|class|\n",
      "+-------+---------------+--------------+---------------+-------------+----------------------+-----------+-----------+-------------+-------+-----+\n",
      "|1000025|              5|             1|              1|            1|                     2|          1|          3|            1|      1|    2|\n",
      "|1002945|              5|             4|              4|            5|                     7|         10|          3|            2|      1|    2|\n",
      "|1015425|              3|             1|              1|            1|                     2|          2|          3|            1|      1|    2|\n",
      "|1016277|              6|             8|              8|            1|                     3|          4|          3|            7|      1|    2|\n",
      "|1017023|              4|             1|              1|            3|                     2|          1|          3|            1|      1|    2|\n",
      "+-------+---------------+--------------+---------------+-------------+----------------------+-----------+-----------+-------------+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|class|count|\n",
      "+-----+-----+\n",
      "|    4|  241|\n",
      "|    2|  458|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "class_count = data.groupBy(col(\"class\")).count()\n",
    "class_count.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting and Filtering Rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+--------------+---------------+-------------+----------------------+-----------+-----------+-------------+-------+-----+\n",
      "|     id|clump_thickness|unif_cell_size|unif_cell_shape|marg_adhesion|single_epith_cell_size|bare_nuclei|bland_chrom|norm_nucleoli|mitoses|class|\n",
      "+-------+---------------+--------------+---------------+-------------+----------------------+-----------+-----------+-------------+-------+-----+\n",
      "|1057013|              8|             4|              5|            1|                     2|       null|          7|            3|      1|    4|\n",
      "|1096800|              6|             6|              6|            9|                     6|       null|          7|            8|      1|    2|\n",
      "|1183246|              1|             1|              1|            1|                     1|       null|          2|            1|      1|    2|\n",
      "|1184840|              1|             1|              3|            1|                     2|       null|          2|            1|      1|    2|\n",
      "|1193683|              1|             1|              2|            1|                     3|       null|          1|            1|      1|    2|\n",
      "|1197510|              5|             1|              1|            1|                     2|       null|          3|            1|      1|    2|\n",
      "|1241232|              3|             1|              4|            1|                     2|       null|          3|            1|      1|    2|\n",
      "| 169356|              3|             1|              1|            1|                     2|       null|          3|            1|      1|    2|\n",
      "| 432809|              3|             1|              3|            1|                     2|       null|          2|            1|      1|    2|\n",
      "| 563649|              8|             8|              8|            1|                     2|       null|          6|           10|      1|    4|\n",
      "| 606140|              1|             1|              1|            1|                     2|       null|          2|            1|      1|    2|\n",
      "|  61634|              5|             4|              3|            1|                     2|       null|          2|            3|      1|    2|\n",
      "| 704168|              4|             6|              5|            6|                     7|       null|          4|            9|      1|    2|\n",
      "| 733639|              3|             1|              1|            1|                     2|       null|          3|            1|      1|    2|\n",
      "|1238464|              1|             1|              1|            1|                     1|       null|          2|            1|      1|    2|\n",
      "|1057067|              1|             1|              1|            1|                     1|       null|          1|            1|      1|    2|\n",
      "+-------+---------------+--------------+---------------+-------------+----------------------+-----------+-----------+-------------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.filter(col(\"bare_nuclei\").isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data.filter(col(\"bare_nuclei\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "683"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "699"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|class|count|\n",
      "+-----+-----+\n",
      "|    4|  239|\n",
      "|    2|  444|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "class_count2 = data2.groupBy(col(\"class\")).count()\n",
    "class_count2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will use data2 (rather than data) in all of the remaining lab, because it does not contain missing/null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 Feature Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StringIndex\n",
    "- Transforms a column of string to a new column of index (type double).\n",
    "- The feature transformation involves three steps:\n",
    "-- Step 1: Create a \"transformer\" \n",
    "-- Step 2: Use the data (which contains all possible values of the string column) to create a mapping (of these strings into an integer/index)\n",
    "-- Step 3: Use the mapping to generate the new column's value (i.e., trasformed index) for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelIndexer= StringIndexer(inputCol=\"class\", outputCol=\"indexedLabel\").fit(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringIndexerModel: uid=StringIndexer_26937aa527b3, handleInvalid=error"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data = labelIndexer.transform(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+--------------+---------------+-------------+----------------------+-----------+-----------+-------------+-------+-----+------------+\n",
      "|     id|clump_thickness|unif_cell_size|unif_cell_shape|marg_adhesion|single_epith_cell_size|bare_nuclei|bland_chrom|norm_nucleoli|mitoses|class|indexedLabel|\n",
      "+-------+---------------+--------------+---------------+-------------+----------------------+-----------+-----------+-------------+-------+-----+------------+\n",
      "|1000025|              5|             1|              1|            1|                     2|          1|          3|            1|      1|    2|         0.0|\n",
      "|1002945|              5|             4|              4|            5|                     7|         10|          3|            2|      1|    2|         0.0|\n",
      "|1015425|              3|             1|              1|            1|                     2|          2|          3|            1|      1|    2|         0.0|\n",
      "|1016277|              6|             8|              8|            1|                     3|          4|          3|            7|      1|    2|         0.0|\n",
      "|1017023|              4|             1|              1|            3|                     2|          1|          3|            1|      1|    2|         0.0|\n",
      "|1017122|              8|            10|             10|            8|                     7|         10|          9|            7|      1|    4|         1.0|\n",
      "|1018099|              1|             1|              1|            1|                     2|         10|          3|            1|      1|    2|         0.0|\n",
      "|1018561|              2|             1|              2|            1|                     2|          1|          3|            1|      1|    2|         0.0|\n",
      "|1033078|              2|             1|              1|            1|                     2|          1|          1|            1|      5|    2|         0.0|\n",
      "|1033078|              4|             2|              1|            1|                     2|          1|          2|            1|      1|    2|         0.0|\n",
      "+-------+---------------+--------------+---------------+-------------+----------------------+-----------+-----------+-------------+-------+-----+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = ['clump_thickness', 'unif_cell_size', 'unif_cell_shape', 'marg_adhesion', \\\n",
    "                  'single_epith_cell_size', 'bare_nuclei', 'bland_chrom', 'norm_nucleoli', 'mitoses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=input_features, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorAssembler_38c6837acae4"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_data = assembler.transform(transformed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will use `vectorized_data` for splitting labelled data into training data and testing data. This way, we have access to all the original features, which we will need for generating decision tree visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|            features|indexedLabel|\n",
      "+--------------------+------------+\n",
      "|[5.0,1.0,1.0,1.0,...|         0.0|\n",
      "|[5.0,4.0,4.0,5.0,...|         0.0|\n",
      "|[3.0,1.0,1.0,1.0,...|         0.0|\n",
      "|[6.0,8.0,8.0,1.0,...|         0.0|\n",
      "|[4.0,1.0,1.0,3.0,...|         0.0|\n",
      "|[8.0,10.0,10.0,8....|         1.0|\n",
      "|[1.0,1.0,1.0,1.0,...|         0.0|\n",
      "|[2.0,1.0,2.0,1.0,...|         0.0|\n",
      "|[2.0,1.0,1.0,1.0,...|         0.0|\n",
      "|[4.0,2.0,1.0,1.0,...|         0.0|\n",
      "+--------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorized2_data = vectorized_data.select(\"features\",'indexedLabel')\n",
    "vectorized2_data.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 Decision Tree Learning and Evaluation (1 hyperparameter setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## randomSplit is a method for DataFrame that split data in the DataFrame into two subsets, one for training, the other for testing, using a number as the seed for random number generator.\n",
    "## If you want to generate a different split, you can use a different seed (preferably a prime number)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData, testData= vectorized_data.randomSplit([0.75, 0.25], seed=1237)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"indexedLabel\", maxDepth=6, minInstancesPerNode=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier_e6ec061e085c"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model = dt.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_e6ec061e085c, depth=6, numNodes=31, numClasses=2, numFeatures=9"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction = dt_model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+--------------+---------------+-------------+----------------------+-----------+-----------+-------------+-------+-----+------------+--------------------+-------------+--------------------+----------+\n",
      "|    id|clump_thickness|unif_cell_size|unif_cell_shape|marg_adhesion|single_epith_cell_size|bare_nuclei|bland_chrom|norm_nucleoli|mitoses|class|indexedLabel|            features|rawPrediction|         probability|prediction|\n",
      "+------+---------------+--------------+---------------+-------------+----------------------+-----------+-----------+-------------+-------+-----+------------+--------------------+-------------+--------------------+----------+\n",
      "|142932|              7|             6|             10|            5|                     3|         10|          9|           10|      2|    4|         1.0|[7.0,6.0,10.0,5.0...|  [1.0,118.0]|[0.00840336134453...|       1.0|\n",
      "|144888|              8|            10|             10|            8|                     5|         10|          7|            8|      1|    4|         1.0|[8.0,10.0,10.0,8....|  [1.0,118.0]|[0.00840336134453...|       1.0|\n",
      "|183913|              1|             2|              2|            1|                     2|          1|          1|            1|      1|    2|         0.0|[1.0,2.0,2.0,1.0,...|  [299.0,0.0]|           [1.0,0.0]|       0.0|\n",
      "+------+---------------+--------------+---------------+-------------+----------------------+-----------+-----------+-------------+-------+-----+------------+--------------------+-------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_prediction.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To compare the actual labels and predicted labels more easily, we can select the following columns.\n",
    "## The `probability` column records the probability for the row to be in the \"zero/benign class\" and the probability to be in the \"one/malignant class\".\n",
    "## The `prediction` column records the predicted label for each row, which is the class with the higher probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------+--------------------+----------+\n",
      "|            features|class|indexedLabel|         probability|prediction|\n",
      "+--------------------+-----+------------+--------------------+----------+\n",
      "|[7.0,6.0,10.0,5.0...|    4|         1.0|[0.00840336134453...|       1.0|\n",
      "|[8.0,10.0,10.0,8....|    4|         1.0|[0.00840336134453...|       1.0|\n",
      "|[1.0,2.0,2.0,1.0,...|    2|         0.0|           [1.0,0.0]|       0.0|\n",
      "|[5.0,3.0,2.0,8.0,...|    4|         1.0|           [1.0,0.0]|       0.0|\n",
      "|[10.0,4.0,4.0,10....|    4|         1.0|[0.04166666666666...|       1.0|\n",
      "+--------------------+-----+------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_prediction.select(\"features\",\"class\",\"indexedLabel\", \"probability\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2', '4']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelIndexer.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelConverter=IndexToString(inputCol=\"prediction\", outputCol=\"predictedClass\", labels=labelIndexer.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2_prediction = labelConverter.transform(test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------+----------+--------------+\n",
      "|            features|class|indexedLabel|prediction|predictedClass|\n",
      "+--------------------+-----+------------+----------+--------------+\n",
      "|[7.0,6.0,10.0,5.0...|    4|         1.0|       1.0|             4|\n",
      "|[8.0,10.0,10.0,8....|    4|         1.0|       1.0|             4|\n",
      "|[1.0,2.0,2.0,1.0,...|    2|         0.0|       0.0|             2|\n",
      "|[5.0,3.0,2.0,8.0,...|    4|         1.0|       0.0|             2|\n",
      "|[10.0,4.0,4.0,10....|    4|         1.0|       1.0|             4|\n",
      "+--------------------+-----+------------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test2_prediction.select(\"features\",\"class\",\"indexedLabel\",\"prediction\",\"predictedClass\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.9338310711681739\n"
     ]
    }
   ],
   "source": [
    "f1 = evaluator.evaluate(test_prediction)\n",
    "print(\"f1 score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 DT Learning Using ML Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Exercise 3: (10 points) In the code cell below, fill in a value for maxDepth (recommend: 2 to 5) and a value of minInstancesPerNode (recommend: 1 to 5). Run the entire sequence of code below to generate a decision tree (using pipeline) and compute f1 measure of the testing data.\n",
    "- After you run the code successfully, record the f1 measure and your choice of max_depth and minInstancesPerNode below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer for Exercise 3: \n",
    "- The f1 measure of testing data for max_detph 6 and minInstancesPerNode 4 = .944"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData, testData= data2.randomSplit([0.8, 0.2], seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelIndexer= StringIndexer(inputCol=\"class\", outputCol=\"indexedLabel\").fit(data2)\n",
    "assembler = VectorAssembler( inputCols=input_features, outputCol=\"features\")\n",
    "dt=DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\", maxDepth=6, minInstancesPerNode=4)\n",
    "predictionConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedClass\", labels=labelIndexer.labels)\n",
    "pipeline = Pipeline(stages=[labelIndexer, assembler, dt, predictionConverter])\n",
    "model = pipeline.fit(trainingData)\n",
    "predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline_d52760a25c6b"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineModel_6c2e61d104a7"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+--------------------+----------+--------------+\n",
      "|class|indexedLabel|            features|prediction|predictedClass|\n",
      "+-----+------------+--------------------+----------+--------------+\n",
      "|    4|         1.0|[10.0,4.0,7.0,2.0...|       1.0|             4|\n",
      "|    4|         1.0|[7.0,6.0,10.0,5.0...|       1.0|             4|\n",
      "|    2|         0.0|[1.0,1.0,1.0,1.0,...|       0.0|             2|\n",
      "|    2|         0.0|[1.0,1.0,3.0,1.0,...|       0.0|             2|\n",
      "|    4|         1.0|[10.0,5.0,10.0,3....|       1.0|             4|\n",
      "|    2|         0.0|[4.0,1.0,1.0,2.0,...|       0.0|             2|\n",
      "|    2|         0.0|[4.0,1.0,1.0,1.0,...|       0.0|             2|\n",
      "|    4|         1.0|[7.0,2.0,4.0,1.0,...|       0.0|             2|\n",
      "|    2|         0.0|[3.0,1.0,1.0,1.0,...|       0.0|             2|\n",
      "|    4|         1.0|[1.0,4.0,3.0,10.0...|       1.0|             4|\n",
      "+-----+------------+--------------------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"class\",\"indexedLabel\",\"features\",\"prediction\",\"predictedClass\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.9439967439967439\n"
     ]
    }
   ],
   "source": [
    "f1 = evaluator.evaluate(predictions)\n",
    "print(\"f1 score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5 Decision Tree Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stages[2] of the pipeline is \"dt\" (DecisionTreeClassifier). \n",
    "## model is a DataFrame representing a trained pipeline.\n",
    "## model.stages[2] gives us the Decision Tree model learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_1c3a5a8a8311, depth=6, numNodes=27, numClasses=2, numFeatures=9\n"
     ]
    }
   ],
   "source": [
    "DTmodel = model.stages[2]\n",
    "print(DTmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: (10 points) \n",
    "- Complete the code below to generate a visualization of the decision tree.\n",
    "- Fill in your PSU ID in the path for ``model_path`` and ``output_path``\n",
    "- Save the visualization of the tree in a file that replaces ??? with your first initial and last name.  For example, I would name the file as \"DTree_jyen_Part5.html\".\n",
    "- Download the HTML file of the tree and submit it as a part of Lab8 assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path=\"DTmodel_vis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tree=decision_tree_parse(DTmodel, ss, model_path)\n",
    "column = dict([(str(idx), i) for idx, i in enumerate(input_features)])\n",
    "plot_trees(tree, column = column, output_path = 'DTree_dwoodford_Part5.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6 Automated Hyperparameter Tuning for Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: (20 points)  \n",
    "- Complete the code below to perform hyper parameter tuning of Decision Tree (for two parameters: max_depth and minInstancesPerNode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = ['clump_thickness', 'unif_cell_size', 'unif_cell_shape', 'marg_adhesion', \\\n",
    "                  'single_epith_cell_size', 'bare_nuclei', 'bland_chrom', 'norm_nucleoli', 'mitoses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData, testingData= data2.randomSplit([0.75, 0.25], seed=1234)\n",
    "model_path=\"DTmodel_vis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/22 20:42:14 WARN CacheManager: Asked to cache already cached data.\n",
      "24/10/22 20:42:14 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best max_depth is  2 , best minInstancesPerNode =  2 , testing f1 =  0.9366854925331762\n"
     ]
    }
   ],
   "source": [
    "## Initialize a Pandas DataFrame to store evaluation results of all combination of hyper-parameter settings\n",
    "hyperparams_eval_df = pd.DataFrame( columns = ['max_depth', 'minInstancesPerNode', 'training f1', 'testing f1', 'Best Model'] )\n",
    "# initialize index to the hyperparam_eval_df to 0\n",
    "index =0 \n",
    "# initialize lowest_error\n",
    "highest_testing_f1 = 0\n",
    "# Set up the possible hyperparameter values to be evaluated\n",
    "max_depth_list = [2, 3, 4, 5]\n",
    "minInstancesPerNode_list = [2, 3, 4]\n",
    "labelIndexer = StringIndexer(inputCol=\"class\", outputCol=\"indexedLabel\").fit(data2)\n",
    "assembler = VectorAssembler( inputCols=input_features, outputCol=\"features\")\n",
    "labelConverter = IndexToString(inputCol = \"prediction\", outputCol=\"predictedClass\", labels=labelIndexer.labels)\n",
    "trainingData.persist()\n",
    "testingData.persist()\n",
    "for max_depth in max_depth_list:\n",
    "    for minInsPN in minInstancesPerNode_list:\n",
    "        seed = 37\n",
    "        # Construct a DT model using a set of hyper-parameter values and training data\n",
    "        dt= DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\", maxDepth= max_depth, minInstancesPerNode= minInst)\n",
    "        pipeline = Pipeline(stages=[labelIndexer, assembler, dt, labelConverter])\n",
    "        model = pipeline.fit(trainingData)\n",
    "        training_predictions = model.transform(trainingData)\n",
    "        testing_predictions = model.transform(testingData)\n",
    "        evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "        training_f1 = evaluator.evaluate(training_predictions)\n",
    "        testing_f1 = evaluator.evaluate(testing_predictions)\n",
    "        # We use 0 as default value of the 'Best Model' column in the Pandas DataFrame.\n",
    "        # The best model will have a value 1000\n",
    "        hyperparams_eval_df.loc[index] = [ max_depth, minInsPN, training_f1, testing_f1, 0]  \n",
    "        index = index +1\n",
    "        if testing_f1 > highest_testing_f1 :\n",
    "            best_max_depth = max_depth\n",
    "            best_minInsPN = minInsPN\n",
    "            best_index = index -1\n",
    "            best_parameters_training_f1 = training_f1\n",
    "            best_DTmodel= model.stages[2]\n",
    "            best_tree = decision_tree_parse(best_DTmodel, ss, model_path)\n",
    "            column = dict( [ (str(idx), i) for idx, i in enumerate(input_features) ])           \n",
    "            highest_testing_f1 = testing_f1\n",
    "print('The best max_depth is ', best_max_depth, ', best minInstancesPerNode = ', \\\n",
    "      best_minInsPN, ', testing f1 = ', highest_testing_f1) \n",
    "column = dict([(str(idx), i) for idx, i in enumerate(input_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path=\"BestDTmodel_dwoodford_Part6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_tree=decision_tree_parse(best_DTmodel, ss, best_model_path)\n",
    "column = dict([(str(idx), i) for idx, i in enumerate(input_features)])\n",
    "plot_trees(best_tree, column = column, output_path = 'BestDTtree_dwoodford_tuned_Part6.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the Testing RMS in the DataFrame\n",
    "hyperparams_eval_df.loc[best_index]=[best_max_depth, best_minInsPN, best_parameters_training_f1, highest_testing_f1, 1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6 (10 points)\n",
    "### Complete the path below to save the result of your hyperparameter tuning in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"Lab8DT_HyperparamsTuningResult_Table.csv\"\n",
    "hyperparams_eval_df.to_csv(output_path)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7 A Revised Hyper-parameter Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/22 20:45:48 WARN CacheManager: Asked to cache already cached data.\n",
      "24/10/22 20:45:48 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best max_depth is  2 , best minInstancesPerNode =  2 , testing f1 =  0.9366854925331762\n"
     ]
    }
   ],
   "source": [
    "## Initialize a Pandas DataFrame to store evaluation results of all combination of hyper-parameter settings\n",
    "hyperparams_eval_df = pd.DataFrame( columns = ['max_depth', 'minInstancesPerNode', 'training f1', 'testing f1', 'Best Model'] )\n",
    "# initialize index to the hyperparam_eval_df to 0\n",
    "index =0 \n",
    "# initialize lowest_error\n",
    "highest_testing_f1 = 0\n",
    "# Set up the possible hyperparameter values to be evaluated\n",
    "max_depth_list = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "minInstancesPerNode_list = [2, 3, 4, 5, 6, 6, 7, 8, 9, 10]\n",
    "labelIndexer = StringIndexer(inputCol=\"class\", outputCol=\"indexedLabel\").fit(data2)\n",
    "assembler = VectorAssembler( inputCols=input_features, outputCol=\"features\")\n",
    "labelConverter = IndexToString(inputCol = \"prediction\", outputCol=\"predictedClass\", labels=labelIndexer.labels)\n",
    "trainingData.persist()\n",
    "testingData.persist()\n",
    "for max_depth in max_depth_list:\n",
    "    for minInsPN in minInstancesPerNode_list:\n",
    "        seed = 37\n",
    "        # Construct a DT model using a set of hyper-parameter values and training data\n",
    "        dt= DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\", maxDepth= max_depth, minInstancesPerNode= minInst)\n",
    "        pipeline = Pipeline(stages=[labelIndexer, assembler, dt, labelConverter])\n",
    "        model = pipeline.fit(trainingData)\n",
    "        training_predictions = model.transform(trainingData)\n",
    "        testing_predictions = model.transform(testingData)\n",
    "        evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "        training_f1 = evaluator.evaluate(training_predictions)\n",
    "        testing_f1 = evaluator.evaluate(testing_predictions)\n",
    "        # We use 0 as default value of the 'Best Model' column in the Pandas DataFrame.\n",
    "        # The best model will have a value 1000\n",
    "        hyperparams_eval_df.loc[index] = [ max_depth, minInsPN, training_f1, testing_f1, 0]  \n",
    "        index = index +1\n",
    "        if testing_f1 > highest_testing_f1 :\n",
    "            best_max_depth = max_depth\n",
    "            best_minInsPN = minInsPN\n",
    "            best_index = index -1\n",
    "            best_parameters_training_f1 = training_f1\n",
    "            best_DTmodel= model.stages[2]\n",
    "            best_tree = decision_tree_parse(best_DTmodel, ss, model_path)\n",
    "            column = dict( [ (str(idx), i) for idx, i in enumerate(input_features) ])           \n",
    "            highest_testing_f1 = testing_f1\n",
    "print('The best max_depth is ', best_max_depth, ', best minInstancesPerNode = ', \\\n",
    "      best_minInsPN, ', testing f1 = ', highest_testing_f1) \n",
    "column = dict([(str(idx), i) for idx, i in enumerate(input_features)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7 (20 points) Copy the code for Part 6 to the the code cells below and modify the range of hyper-parameter tuning into the following:\n",
    "- max_depth: 2 to 11\n",
    "- minInstancesPerNode: 2 to 10\n",
    "## Modify the file names of your Decision Tree visualization files (.html) so that you do not accidentally destroy the visualiztion generated for Exercise 5 (Part 6).\n",
    "## Modify the file names of your output files so that you can compare the results of Part 7 (both decision tree and best hyper parameters) with those of Part 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path=\"BestDTmodel_dwoodford_Part7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tree=decision_tree_parse(best_DTmodel, ss, best_model_path)\n",
    "column = dict([(str(idx), i) for idx, i in enumerate(input_features)])\n",
    "plot_trees(best_tree, column = column, output_path = 'BestDTtree_dwoodford_tuned_Part7.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams_eval_df.loc[best_index]=[best_max_depth, best_minInsPN, best_parameters_training_f1, highest_testing_f1, 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"Lab8DT_HyperparamsTuningResult_Table2.csv\"\n",
    "hyperparams_eval_df.to_csv(output_path)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8 (10 points) Compare (a) the hyper-parameters and (b) the decision trees generated by Part 6 and Part 7. Discuss the results in the Markdown cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer for Exercise 8: \n",
    "- (a) The best max depth and minimum instances per node were the same for both trees, with the value of 2 for all of our hyperparameters.\n",
    "- (b) Both trees were exactly the same as well, using unif_cell_shape, bland_chrom, and unif_cell_size to split the data, and making the same prediction based on the same range of values.\n",
    "\n",
    "I believe the model is easily overfitting this data, with training f1 scores going up for higher complexity and testing f1 scores going down for higher complexity. The simplist model won because of its ability to test the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
